{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver = './chromedriver'\n",
    "def get_web_page2(url):\n",
    "    from selenium import webdriver\n",
    "    driver = webdriver.Chrome(chromedriver) \n",
    "    driver.get(url)\n",
    "    time.sleep(3) \n",
    "    html = driver.page_source \n",
    "    driver.close() \n",
    "    \n",
    "    return(BeautifulSoup(html,'html.parser'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detail_page_sale(url):\n",
    "    soup = get_web_page2(url)\n",
    "    obj = soup.find_all('div',attrs={'class':'j-house houseList-item clearfix z-hastag'})\n",
    "    \n",
    "    result = pd.DataFrame(columns=['head', 'age', 'section', 'addr', 'shape', 'price', 'unit_price', 'floor', 'layout', \n",
    "                                   'area', 'main_area', 'community','link'])\n",
    "    for oo in obj:\n",
    "        head = oo.find('div',attrs={'class':'houseList-item-title'}).a.contents[0]\n",
    "        link = 'https://sale.591.com.tw' + oo.find('div',attrs={'class':'houseList-item-title'}).a['href']\n",
    "        price = int(''.join(oo.find('div',attrs={'class':'houseList-item-price'}).em.contents[0].split(',')))\n",
    "        unit_price = oo.find('div',attrs={'class':'houseList-item-unitprice'}).contents[0]\n",
    "        \n",
    "        shape = np.nan; layout = np.nan; area = np.nan; main_area = np.nan; age = np.nan; floor=np.nan;\n",
    "        community = np.nan; section = np.nan; addr = np.nan;\n",
    "        \n",
    "        tmp1 = oo.find_all('div',attrs={'class':'houseList-item-attr-row clearfix'})\n",
    "        for t in tmp1:\n",
    "            if t.find('span', attrs={'class':'houseList-item-attrs-shape'}) is not None:\n",
    "                shape = t.find('span', attrs={'class':'houseList-item-attrs-shape'}).contents[0]\n",
    "            if t.find('span', attrs={'class':'houseList-item-attrs-layout'}) is not None:\n",
    "                layout = t.find('span', attrs={'class':'houseList-item-attrs-layout'}).contents[0]\n",
    "            if t.find('span', attrs={'class':'houseList-item-attrs-area'}) is not None:\n",
    "                area = t.find('span', attrs={'class':'houseList-item-attrs-area'}).contents[0]\n",
    "            if t.find('span', attrs={'class':\"houseList-item-attrs-mainarea\"}) is not None:\n",
    "                main_area = t.find('span', attrs={'class':'houseList-item-attrs-mainarea'}).contents[0]\n",
    "            if t.find('span', attrs={'class':'houseList-item-attrs-houseage'}) is not None:\n",
    "                age = t.find('span', attrs={'class':'houseList-item-attrs-houseage'}).contents[0]\n",
    "            if t.find('span', attrs={'class':'houseList-item-attrs-floor'}) is not None:\n",
    "                floor = t.find('span', attrs={'class':'houseList-item-attrs-floor'}).contents[0]\n",
    "        \n",
    "        tmp2 = oo.find_all('div',attrs={'class':'houseList-item-address-row clearfix'})\n",
    "        for t2 in tmp2:\n",
    "            if t2.find('span', attrs={'class':'houseList-item-community mr15 pull-left'}) is not None:\n",
    "                if t2.find('span', attrs={'class':'houseList-item-community mr15 pull-left'}).a is not None:\n",
    "                    community = t2.find('span', attrs={'class':'houseList-item-community mr15 pull-left'}).a.contents[0]\n",
    "            if t2.find('span', attrs={'class':'houseList-item-section pull-left'}) is not None:\n",
    "                section = t2.find('span', attrs={'class':'houseList-item-section pull-left'}).contents[0]\n",
    "            if t2.find('span', attrs={'class':'houseList-item-address pull-left'}) is not None:\n",
    "                addr = t2.find('span', attrs={'class':'houseList-item-address pull-left'}).contents[0]\n",
    "        \n",
    "        result = pd.concat([result, \n",
    "                            pd.DataFrame([[head, age, section, addr, shape, price, unit_price, floor, layout, area, \n",
    "                                           main_area, community, link]],\n",
    "                            columns=['head', 'age', 'section', 'addr', 'shape', 'price', 'unit_price', 'floor', 'layout', \n",
    "                                     'area', 'main_area', 'community','link'])],0)\n",
    "    return(result.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler_main2(url):\n",
    "    soup = get_web_page2(url)\n",
    "    ttlnum = int(soup.find('a', attrs={'class':'pageNum-form'})['data-total'])\n",
    "    ttlpage = int(ttlnum/30)+1\n",
    "    \n",
    "    result = pd.DataFrame(columns=['head', 'age', 'section', 'addr', 'shape', 'price', 'unit_price', 'floor', 'layout', \n",
    "                                   'area', 'main_area', 'community','link'])\n",
    "    \n",
    "    print('Processing Pages ({} Pages In Total): '.format(ttlpage))\n",
    "    for l in range(ttlpage):\n",
    "        print('{}'.format(l), end='\\t')\n",
    "        url2 = url + '&firstRow={}'.format(l*30)\n",
    "        result = detail_page_sale(url2)\n",
    "        #result = pd.concat([result, tmp], 0)\n",
    "        result.to_csv('tpe_v2.csv',index=False, mode='a')\n",
    "        #time.sleep(1)\n",
    "        \n",
    "    \n",
    "    print('\\nProcess Done!!\\n')\n",
    "    return(result.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taipei Sale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://sale.591.com.tw/?shType=list&regionid=1'\n",
    "taipei_sale = crawler_main2(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
